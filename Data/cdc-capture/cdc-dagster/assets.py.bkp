# import os
# import pandas as pd
# from dagster import asset, AssetExecutionContext, Definitions, EnvVar, Config
# from dagster_deltalake import LocalConfig
# from dagster_deltalake_pandas import DeltaLakePandasIOManager
# import sqlalchemy as sa
# from sqlalchemy import create_engine


# class SQLServerCDCConfig(Config):
#     """Configuration for SQL Server CDC connection.
#     Note: use EnvVar to keep them secret in the UI."""

#     host: EnvVar = EnvVar("SQL_SERVER_HOST")
#     user: EnvVar = EnvVar("SQL_SERVER_USER")
#     password: EnvVar = EnvVar("SQL_SERVER_PASSWORD")
#     database: EnvVar = EnvVar("SQL_SERVER_DATABASE")
#     table_name: str
#     last_lsn: str = "0x00000000000000000000"

#     def get_connection_string(self):
#         """Construct the connection string for SQLAlchemy."""
#         return f"mssql+pyodbc://{self.user.get_value()}:{self.password.get_value()}@{self.host.get_value()}/{self.database.get_value()}?driver=ODBC+Driver+17+for+SQL+Server"


# class SQLServerCDCResource:
#     """Resource to handle SQL Server CDC operations."""

#     def __init__(self, config: SQLServerCDCConfig):
#         self.config = config
#         self.engine = create_engine(self.config.get_connection_string())

#     def get_table_changes(self, table_name, last_lsn):
#         """Get changes from a CDC-enabled table since the last LSN."""
#         connection = self.engine.connect()

#         try:
#             # Get the capture instance name
#             capture_instance_query = f"""
#                 SELECT capture_instance
#                 FROM cdc.change_tables
#                 WHERE source_schema = 'dbo'
#                 AND source_name = '{table_name}'
#             """
#             capture_instance = connection.execute(
#                 sa.text(capture_instance_query)
#             ).scalar()

#             if not capture_instance:
#                 raise ValueError(f"CDC not enabled for table {table_name}")

#             # Get current maximum LSN
#             current_lsn_query = "SELECT sys.fn_cdc_get_max_lsn()"
#             current_lsn = connection.execute(sa.text(current_lsn_query)).scalar()

#             # Get changes since last LSN
#             changes_query = f"""
#                 SELECT *
#                 FROM cdc.fn_cdc_get_all_changes_{capture_instance}(
#                     CAST('{last_lsn}' AS BINARY(10)),
#                     CAST('{current_lsn}' AS BINARY(10)),
#                     'all'
#                 )
#             """
#             changes_df = pd.read_sql(sa.text(changes_query), connection)

#             return changes_df, current_lsn

#         finally:
#             connection.close()

#     def get_full_table_data(self, table_name):
#         """Get the entire table data (used for initial load)."""
#         query = f"SELECT * FROM {table_name}"
#         return pd.read_sql(query, self.engine)


# # Assets to track and persist CDC changes to Delta Lake
# @asset(
#     group_name="cdc",
#     io_manager_key="delta_io_manager",
# )
# def customers_delta(
#     context: AssetExecutionContext, sql_server_cdc: SQLServerCDCResource
# ) -> pd.DataFrame:
#     """Asset that tracks changes to the Customers table and writes to Delta Lake."""
#     table_name = "Customers"

#     # Get the last processed LSN from metadata
#     last_lsn = context.instance.get_dynamic_partitions(
#         partition_key="last_lsn", asset_key=context.asset_key
#     )
#     if not last_lsn:
#         last_lsn = ["0x00000000000000000000"]

#     context.log.info(f"Processing changes for {table_name} since LSN: {last_lsn[0]}")

#     try:
#         # Get changes since last processed LSN
#         changes_df, current_lsn = sql_server_cdc.get_table_changes(
#             table_name, last_lsn[0]
#         )

#         # If no existing delta file or first run, get full data
#         delta_path = f"/tmp/io_manager_storage/delta/{table_name.lower()}"
#         if last_lsn[0] == "0x00000000000000000000" or not os.path.exists(delta_path):
#             context.log.info(f"Performing initial load for {table_name}")
#             full_df = sql_server_cdc.get_full_table_data(table_name)
#             context.instance.add_dynamic_partitions(
#                 partition_key="last_lsn",
#                 partitions=[str(current_lsn)],
#             )
#             return full_df

#         # Process CDC changes if any
#         if not changes_df.empty:
#             context.log.info(f"Found {len(changes_df)} changes for {table_name}")

#             # Read current Delta table
#             try:
#                 current_data = pd.read_parquet(delta_path)
#             except Exception as e:
#                 context.log.error(f"Error reading current delta file: {e}")
#                 current_data = pd.DataFrame()

#             # Process the changes based on operation type
#             for _, change in changes_df.iterrows():
#                 operation = change["__$operation"]
#                 customer_id = change["CustomerID"]

#                 if operation == 1:  # Delete
#                     if not current_data.empty:
#                         current_data = current_data[
#                             current_data["CustomerID"] != customer_id
#                         ]

#                 elif operation == 2:  # Insert
#                     # Create a new row from the CDC change
#                     new_row = {
#                         col: change[col]
#                         for col in change.index
#                         if not col.startswith("__$")
#                     }
#                     current_data = pd.concat(
#                         [current_data, pd.DataFrame([new_row])], ignore_index=True
#                     )

#                 elif operation in (3, 4):  # Update (before or after)
#                     if operation == 4:  # After update
#                         # Remove the old record
#                         if not current_data.empty:
#                             current_data = current_data[
#                                 current_data["CustomerID"] != customer_id
#                             ]
#                         # Add the new record
#                         new_row = {
#                             col: change[col]
#                             for col in change.index
#                             if not col.startswith("__$")
#                         }
#                         current_data = pd.concat(
#                             [current_data, pd.DataFrame([new_row])], ignore_index=True
#                         )

#             # Update the last processed LSN
#             context.instance.add_dynamic_partitions(
#                 partition_key="last_lsn",
#                 partitions=[str(current_lsn)],
#             )

#             return current_data
#         else:
#             context.log.info(
#                 f"No changes found for {table_name} since LSN {last_lsn[0]}"
#             )
#             # Return the current data from Delta Lake
#             try:
#                 return pd.read_parquet(delta_path)
#             except Exception as e:
#                 context.log.error(f"Error reading current delta file: {e}")
#                 return pd.DataFrame()

#     except Exception as e:
#         context.log.error(f"Error processing CDC changes: {e}")
#         # In case of error, try to return the current data
#         try:
#             return pd.read_parquet(delta_path)
#         except:
#             return pd.DataFrame()


# # Define a similar asset for Orders table
# @asset(
#     group_name="cdc",
#     io_manager_key="delta_io_manager",
# )
# def orders_delta(
#     context: AssetExecutionContext, sql_server_cdc: SQLServerCDCResource
# ) -> pd.DataFrame:
#     """Asset that tracks changes to the Orders table and writes to Delta Lake."""
#     # Similar implementation as customers_delta but for Orders table
#     table_name = "Orders"

#     # Implementation would be very similar to the customers_delta asset
#     # For brevity, the implementation details are omitted here
#     # In a real implementation, you would duplicate the logic from customers_delta
#     # but adapt it for the Orders table and its primary key (OrderID)

#     # This is a placeholder to demonstrate the concept
#     return pd.DataFrame()


# # Define Dagster Definitions
# defs = Definitions(
#     assets=[customers_delta, orders_delta],
#     resources={
#         "sql_server_cdc": SQLServerCDCResource(
#             SQLServerCDCConfig(table_name="Customers")
#         ),
#         "delta_io_manager": DeltaLakePandasIOManager(
#             root_uri="/tmp/io_manager_storage/delta",
#             storage_options=LocalConfig(),
#         ),
#     },
# )
