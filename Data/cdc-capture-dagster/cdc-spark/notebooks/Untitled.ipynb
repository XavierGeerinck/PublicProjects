{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d3ca3b-579a-4536-87d4-7bc5d1d3ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pyodbc polars grpcio-status jupysql --quiet\n",
    "!apt update > /dev/null; apt install -y -q unixodbc > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12f4754e-7f48-4390-8694-0ea6741066db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Local Spark Session...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"Getting Local Spark Session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # Stop the local one\n",
    "# SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n",
    "# print(\"Local Spark session stopped successfully.\")\n",
    "\n",
    "# # Create a remote one\n",
    "# print(\"Starting remote Spark session...\")\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Jupyter\") \\\n",
    "#     .remote(\"sc://localhost:15002\") \\\n",
    "#     .getOrCreate()\n",
    "# print(\"Created remote spark session successfully at sc://localhost:15002\")\n",
    "\n",
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530da01d-9a5e-4581-8c5c-c36610362a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME_REMOTE=\"Customers\"\n",
    "TABLE_NAME_LOCAL=f\"local.db.{TABLE_NAME_REMOTE.lower()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb931ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Import table structure from existing table\n",
    "SQL_HOST=os.environ[\"SQL_HOST\"]\n",
    "SQL_USER=os.environ[\"SQL_USER\"]\n",
    "SQL_PASS=os.environ[\"SQL_PASS\"]\n",
    "SQL_PORT=1433\n",
    "SQL_DATABASE=os.environ[\"SQL_DATABASE\"]\n",
    "\n",
    "# Construct JDBC URL\n",
    "jdbc_url = f\"jdbc:sqlserver://{SQL_HOST}:{SQL_PORT};databaseName={SQL_DATABASE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ab0a3b-8b1b-48fa-833e-882dd957dad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full load successful! 11 rows loaded into Iceberg.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "try:\n",
    "    # First create a temporary table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW jdbc\n",
    "    USING org.apache.spark.sql.jdbc\n",
    "    OPTIONS (\n",
    "        url '{jdbc_url}',\n",
    "        dbtable '{TABLE_NAME_REMOTE}',\n",
    "        user '{SQL_USER}',\n",
    "        password '{SQL_PASS}',\n",
    "        driver 'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Copy the full table from MSSQL\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {TABLE_NAME_LOCAL}\n",
    "        USING iceberg\n",
    "        AS SELECT * FROM jdbc\n",
    "    \"\"\")\n",
    "\n",
    "    # Validate row counts\n",
    "    source_count = spark.sql(\"SELECT COUNT(*) FROM jdbc\").collect()[0][0]\n",
    "    iceberg_count = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME_LOCAL}\").collect()[0][0]\n",
    "\n",
    "    if source_count == iceberg_count:\n",
    "        print(f\"Full load successful! {source_count} rows loaded into Iceberg.\")\n",
    "    else:\n",
    "        print(f\"Row count mismatch: Source({source_count}) vs Iceberg({iceberg_count})\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Import failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4215175c-ed47-4c49-aff1-832075af4b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 13:15:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>CustomerID</th>\n",
       "            <th>FirstName</th>\n",
       "            <th>LastName</th>\n",
       "            <th>Email</th>\n",
       "            <th>CreatedDate</th>\n",
       "            <th>UpdatedDate</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>12</td>\n",
       "            <td>John</td>\n",
       "            <td>Smith</td>\n",
       "            <td>john.smith@example.com</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13</td>\n",
       "            <td>Jane</td>\n",
       "            <td>Doe</td>\n",
       "            <td>jane.doe@example.com</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14</td>\n",
       "            <td>Bob</td>\n",
       "            <td>Johnson</td>\n",
       "            <td>bob.johnson@example.com</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15</td>\n",
       "            <td>Alice</td>\n",
       "            <td>Williams</td>\n",
       "            <td>alice.williams@example.com</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>16</td>\n",
       "            <td>Charlie</td>\n",
       "            <td>Brown</td>\n",
       "            <td>charlie.brown@example.com</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "            <td>2025-03-31 12:06:49.250000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>17</td>\n",
       "            <td>Michael</td>\n",
       "            <td>Taylor</td>\n",
       "            <td>michael.taylor@example.com</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>18</td>\n",
       "            <td>Sarah</td>\n",
       "            <td>Johnson</td>\n",
       "            <td>sarah.johnson@example.com</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>19</td>\n",
       "            <td>David</td>\n",
       "            <td>Miller</td>\n",
       "            <td>david.miller@example.com</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>20</td>\n",
       "            <td>Emma</td>\n",
       "            <td>Wilson</td>\n",
       "            <td>emma.wilson@example.com</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>21</td>\n",
       "            <td>Thomas</td>\n",
       "            <td>Anderson</td>\n",
       "            <td>thomas.anderson@example.com</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "            <td>2025-03-31 12:07:53.450000</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>22</td>\n",
       "            <td>Brave</td>\n",
       "            <td>Sunny</td>\n",
       "            <td>brave.sunny@example.com</td>\n",
       "            <td>2025-03-31 13:16:07.863000</td>\n",
       "            <td>2025-03-31 13:16:07.863000</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------+-----------+----------+-----------------------------+----------------------------+----------------------------+\n",
       "| CustomerID | FirstName | LastName |                       Email |                CreatedDate |                UpdatedDate |\n",
       "+------------+-----------+----------+-----------------------------+----------------------------+----------------------------+\n",
       "|         12 |      John |    Smith |      john.smith@example.com | 2025-03-31 12:06:49.250000 | 2025-03-31 12:06:49.250000 |\n",
       "|         13 |      Jane |      Doe |        jane.doe@example.com | 2025-03-31 12:06:49.250000 | 2025-03-31 12:06:49.250000 |\n",
       "|         14 |       Bob |  Johnson |     bob.johnson@example.com | 2025-03-31 12:06:49.250000 | 2025-03-31 12:06:49.250000 |\n",
       "|         15 |     Alice | Williams |  alice.williams@example.com | 2025-03-31 12:06:49.250000 | 2025-03-31 12:06:49.250000 |\n",
       "|         16 |   Charlie |    Brown |   charlie.brown@example.com | 2025-03-31 12:06:49.250000 | 2025-03-31 12:06:49.250000 |\n",
       "|         17 |   Michael |   Taylor |  michael.taylor@example.com | 2025-03-31 12:07:53.450000 | 2025-03-31 12:07:53.450000 |\n",
       "|         18 |     Sarah |  Johnson |   sarah.johnson@example.com | 2025-03-31 12:07:53.450000 | 2025-03-31 12:07:53.450000 |\n",
       "|         19 |     David |   Miller |    david.miller@example.com | 2025-03-31 12:07:53.450000 | 2025-03-31 12:07:53.450000 |\n",
       "|         20 |      Emma |   Wilson |     emma.wilson@example.com | 2025-03-31 12:07:53.450000 | 2025-03-31 12:07:53.450000 |\n",
       "|         21 |    Thomas | Anderson | thomas.anderson@example.com | 2025-03-31 12:07:53.450000 | 2025-03-31 12:07:53.450000 |\n",
       "|         22 |     Brave |    Sunny |     brave.sunny@example.com | 2025-03-31 13:16:07.863000 | 2025-03-31 13:16:07.863000 |\n",
       "+------------+-----------+----------+-----------------------------+----------------------------+----------------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM $TABLE_NAME_LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44972ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package packages-microsoft-prod.\n",
      "(Reading database ... 34662 files and directories currently installed.)\n",
      "Preparing to unpack packages-microsoft-prod.deb ...\n",
      "Unpacking packages-microsoft-prod (1.0-debian11.1) ...\n",
      "Setting up packages-microsoft-prod (1.0-debian11.1) ...\n",
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Hit:2 http://deb.debian.org/debian-security bullseye-security InRelease\n",
      "Hit:3 http://deb.debian.org/debian bullseye-updates InRelease\n",
      "Get:4 https://packages.microsoft.com/debian/11/prod bullseye InRelease [3650 B]\n",
      "Get:5 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 Packages [42.3 kB]\n",
      "Get:6 https://packages.microsoft.com/debian/11/prod bullseye/main all Packages [1444 B]\n",
      "Get:7 https://packages.microsoft.com/debian/11/prod bullseye/main armhf Packages [38.1 kB]\n",
      "Get:8 https://packages.microsoft.com/debian/11/prod bullseye/main amd64 Packages [192 kB]\n",
      "Fetched 278 kB in 0s (856 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  msodbcsql18\n",
      "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
      "Need to get 688 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 msodbcsql18 arm64 18.5.1.1-1 [688 kB]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 688 kB in 0s (2969 kB/s)\n",
      "Selecting previously unselected package msodbcsql18.\n",
      "(Reading database ... 34670 files and directories currently installed.)\n",
      "Preparing to unpack .../msodbcsql18_18.5.1.1-1_arm64.deb ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Unpacking msodbcsql18 (18.5.1.1-1) ...\n",
      "Setting up msodbcsql18 (18.5.1.1-1) ...\n",
      "odbcinst: Driver installed. Usage count increased to 1. \n",
      "    Target directory is /etc\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  mssql-tools18\n",
      "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
      "Need to get 208 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 mssql-tools18 arm64 18.4.1.1-1 [208 kB]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 208 kB in 1s (146 kB/s)\n",
      "Selecting previously unselected package mssql-tools18.\n",
      "(Reading database ... 34689 files and directories currently installed.)\n",
      "Preparing to unpack .../mssql-tools18_18.4.1.1-1_arm64.deb ...\n",
      "Unpacking mssql-tools18 (18.4.1.1-1) ...\n",
      "Setting up mssql-tools18 (18.4.1.1-1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libodbc1 odbcinst odbcinst1debian2 unixodbc\n",
      "Suggested packages:\n",
      "  msodbcsql17 unixodbc-bin\n",
      "The following NEW packages will be installed:\n",
      "  unixodbc-dev\n",
      "The following packages will be upgraded:\n",
      "  libodbc1 odbcinst odbcinst1debian2 unixodbc\n",
      "4 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
      "Need to get 752 kB of archives.\n",
      "After this operation, 1751 kB of additional disk space will be used.\n",
      "Get:1 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 libodbc1 arm64 2.3.11-1 [495 kB]\n",
      "Get:2 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 odbcinst1debian2 arm64 2.3.11-1 [140 kB]\n",
      "Get:3 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 odbcinst arm64 2.3.11-1 [21.8 kB]\n",
      "Get:4 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 unixodbc arm64 2.3.11-1 [52.5 kB]\n",
      "Get:5 https://packages.microsoft.com/debian/11/prod bullseye/main arm64 unixodbc-dev arm64 2.3.11-1 [42.3 kB]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 752 kB in 0s (2280 kB/s)\n",
      "(Reading database ... 34703 files and directories currently installed.)\n",
      "Preparing to unpack .../libodbc1_2.3.11-1_arm64.deb ...\n",
      "Unpacking libodbc1:arm64 (2.3.11-1) over (2.3.6-0.1+b1) ...\n",
      "Preparing to unpack .../odbcinst1debian2_2.3.11-1_arm64.deb ...\n",
      "Unpacking odbcinst1debian2:arm64 (2.3.11-1) over (2.3.6-0.1+b1) ...\n",
      "Preparing to unpack .../odbcinst_2.3.11-1_arm64.deb ...\n",
      "Unpacking odbcinst (2.3.11-1) over (2.3.6-0.1+b1) ...\n",
      "Preparing to unpack .../unixodbc_2.3.11-1_arm64.deb ...\n",
      "Unpacking unixodbc (2.3.11-1) over (2.3.6-0.1+b1) ...\n",
      "Selecting previously unselected package unixodbc-dev.\n",
      "Preparing to unpack .../unixodbc-dev_2.3.11-1_arm64.deb ...\n",
      "Unpacking unixodbc-dev (2.3.11-1) ...\n",
      "Setting up libodbc1:arm64 (2.3.11-1) ...\n",
      "Setting up odbcinst (2.3.11-1) ...\n",
      "Setting up odbcinst1debian2:arm64 (2.3.11-1) ...\n",
      "Setting up unixodbc (2.3.11-1) ...\n",
      "Setting up unixodbc-dev (2.3.11-1) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u11) ...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "libgssapi-krb5-2 is already the newest version (1.18.3-6+deb11u6).\n",
      "libgssapi-krb5-2 set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install ODBC 18\n",
    "if ! [[ \"11 12\" == *\"$(grep VERSION_ID /etc/os-release | cut -d '\"' -f 2 | cut -d '.' -f 1)\"* ]];\n",
    "then\n",
    "    echo \"Debian $(grep VERSION_ID /etc/os-release | cut -d '\"' -f 2 | cut -d '.' -f 1) is not currently supported.\";\n",
    "    exit;\n",
    "fi\n",
    "\n",
    "# Download the package to configure the Microsoft repo\n",
    "curl -sSL -O https://packages.microsoft.com/config/debian/$(grep VERSION_ID /etc/os-release | cut -d '\"' -f 2 | cut -d '.' -f 1)/packages-microsoft-prod.deb\n",
    "# Install the package\n",
    "sudo dpkg -i packages-microsoft-prod.deb\n",
    "# Delete the file\n",
    "rm packages-microsoft-prod.deb\n",
    "\n",
    "sudo apt-get update\n",
    "sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18\n",
    "# optional: for bcp and sqlcmd\n",
    "sudo ACCEPT_EULA=Y apt-get install -y mssql-tools18\n",
    "echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "# optional: for unixODBC development headers\n",
    "sudo apt-get install -y unixodbc-dev\n",
    "# optional: kerberos library for debian-slim distributions\n",
    "sudo apt-get install -y libgssapi-krb5-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15b783-c52f-47fb-8164-7a171d012982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'polars.dataframe.frame.DataFrame'>\n",
      "+--------------------+--------------------+------------+--------------+----------+---------+--------+--------------------+--------------------+--------------------+\n",
      "|        __$start_lsn|           __$seqval|__$operation|__$update_mask|CustomerID|FirstName|LastName|               Email|         CreatedDate|         UpdatedDate|\n",
      "+--------------------+--------------------+------------+--------------+----------+---------+--------+--------------------+--------------------+--------------------+\n",
      "|[00 00 00 44 00 0...|[00 00 00 44 00 0...|           2|          [3F]|        23|  Michael|  Taylor|michael.taylor@ex...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|[00 00 00 44 00 0...|[00 00 00 44 00 0...|           2|          [3F]|        24|    Sarah| Johnson|sarah.johnson@exa...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|[00 00 00 44 00 0...|[00 00 00 44 00 0...|           2|          [3F]|        25|    David|  Miller|david.miller@exam...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|[00 00 00 44 00 0...|[00 00 00 44 00 0...|           2|          [3F]|        26|     Emma|  Wilson|emma.wilson@examp...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|[00 00 00 44 00 0...|[00 00 00 44 00 0...|           2|          [3F]|        27|   Thomas|Anderson|thomas.anderson@e...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           2|          [3F]|        28|    Brave|   Sunny|brave.sunny@examp...|2025-04-03 13:55:...|2025-04-03 13:55:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           1|          [3F]|        18|    Sarah| Johnson|sarah.johnson@exa...|2025-03-31 12:07:...|2025-03-31 12:07:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           1|          [3F]|        24|    Sarah| Johnson|sarah.johnson@exa...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           2|          [3F]|        29|  Michael|  Taylor|michael.taylor@ex...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           2|          [3F]|        30|    Sarah| Johnson|sarah.johnson@exa...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           2|          [3F]|        31|    David|  Miller|david.miller@exam...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           2|          [3F]|        32|     Emma|  Wilson|emma.wilson@examp...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|[00 00 00 45 00 0...|[00 00 00 45 00 0...|           2|          [3F]|        33|   Thomas|Anderson|thomas.anderson@e...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "+--------------------+--------------------+------------+--------------+----------+---------+--------+--------------------+--------------------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Fetch SQL Server CDC changes from Remote and merge them into the local Iceberg table\n",
    "# we use pyodbc for this (to avoid temporary views)\n",
    "import pyodbc\n",
    "import sqlalchemy as sa\n",
    "import polars as pl\n",
    "from urllib.parse import quote_plus\n",
    "from contextlib import contextmanager\n",
    "\n",
    "LSN_DEFAULT = \"0x00000000000000000000\"\n",
    "\n",
    "class SQLResource:\n",
    "    def __init__(self, host, db, username, password, last_lsn=None):\n",
    "        self.host = host\n",
    "        self.db = db\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "        self.engine = sa.create_engine(\n",
    "            self.get_connection_string(host, db, username, password)\n",
    "        )\n",
    "\n",
    "    def get_connection_string(self, host, db, username, password):\n",
    "        \"\"\"Construct the connection string for SQLAlchemy.\"\"\"\n",
    "        pass_escaped = quote_plus(password)\n",
    "        user_escaped = quote_plus(username)\n",
    "        driver_escaped = quote_plus(\"ODBC Driver 18 for SQL Server\")\n",
    "        return f\"mssql+pyodbc://{user_escaped}:{pass_escaped}@{host}/{db}?driver={driver_escaped}\"\n",
    "\n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a database connection using context manager for automatic cleanup.\"\"\"\n",
    "        connection = self.engine.connect()\n",
    "\n",
    "        try:\n",
    "            yield connection\n",
    "        finally:\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "    def get_primary_key_columns(self, table_name: str) -> list[str]:\n",
    "        \"\"\"Get the primary key columns for a CDC-enabled table.\"\"\"\n",
    "        with self.get_connection() as connection:\n",
    "            instance = self.get_capture_instance_name(\"dbo\", table_name)\n",
    "\n",
    "            query = sa.text(\"\"\"\n",
    "            SELECT column_name FROM cdc.index_columns WHERE object_id = (\n",
    "                SELECT object_id FROM cdc.change_tables WHERE capture_instance = :capture_instance_name\n",
    "            )\n",
    "            \"\"\")\n",
    "\n",
    "            result = connection.execute(query, {\"capture_instance_name\": instance})\n",
    "            primary_key_columns = [row[0] for row in result]\n",
    "            return primary_key_columns\n",
    "\n",
    "    def is_cdc_enabled_for_database(self):\n",
    "        \"\"\"Check if CDC is enabled for the database.\"\"\"\n",
    "        with self.get_connection() as connection:\n",
    "            query = sa.text(\"\"\"\n",
    "                SELECT is_cdc_enabled \n",
    "                FROM sys.databases \n",
    "                WHERE name = :db_name\n",
    "            \"\"\")\n",
    "            result = connection.execute(\n",
    "                query, {\"db_name\": self.db}\n",
    "            ).scalar()\n",
    "            return bool(result)\n",
    "\n",
    "    def is_cdc_enabled_for_table(self, schema_name, table_name):\n",
    "        \"\"\"Check if CDC is enabled for the specified table.\"\"\"\n",
    "        with self.get_connection() as connection:\n",
    "            capture_instance_name = self.get_capture_instance_name(\n",
    "                schema_name, table_name\n",
    "            )\n",
    "\n",
    "            query = sa.text(f\"\"\"\n",
    "                SELECT 1\n",
    "                FROM cdc.change_tables\n",
    "                WHERE capture_instance = '{capture_instance_name}'\n",
    "            \"\"\")\n",
    "\n",
    "            result = connection.execute(\n",
    "                query, {\"schema_name\": schema_name, \"table_name\": table_name}\n",
    "            ).scalar()\n",
    "\n",
    "            return bool(result)\n",
    "\n",
    "    def get_capture_instance_name(self, schema_name, table_name):\n",
    "        \"\"\"Get the CDC capture instance name for a table.\"\"\"\n",
    "        return f\"dbo_{table_name}\"\n",
    "\n",
    "    def get_current_lsn(self):\n",
    "        \"\"\"Get the current  LSN from SQL Server using native function.\"\"\"\n",
    "        with self.get_connection() as connection:\n",
    "            query = sa.text(\"SELECT sys.fn_cdc_get_max_lsn()\")\n",
    "            return connection.execute(query).scalar()\n",
    "\n",
    "    def get_min_lsn(self, capture_instance=None):\n",
    "        \"\"\"Get the minimum available LSN for a capture instance.\"\"\"\n",
    "        with self.get_connection() as connection:\n",
    "            query = sa.text(\"SELECT sys.fn_cdc_get_min_lsn(:capture_instance)\")\n",
    "            return connection.execute(\n",
    "                query, {\"capture_instance\": capture_instance}\n",
    "            ).scalar()\n",
    "\n",
    "    def hex_string_to_lsn(self, lsn_hex):\n",
    "        \"\"\"Convert a hexadecimal LSN string to binary for SQL Server functions.\"\"\"\n",
    "        with self.get_connection() as connection:\n",
    "            if not lsn_hex or not isinstance(lsn_hex, str):\n",
    "                # Return minimum LSN if input is invalid\n",
    "                query = sa.text(\"SELECT sys.fn_cdc_get_min_lsn(NULL)\")\n",
    "                return connection.execute(query).scalar()\n",
    "\n",
    "            if not lsn_hex.startswith(\"0x\"):\n",
    "                lsn_hex = f\"0x{lsn_hex}\"\n",
    "\n",
    "            query = sa.text(\"SELECT CAST(:lsn_hex AS BINARY(10))\")\n",
    "            result = connection.execute(query, {\"lsn_hex\": lsn_hex}).scalar()\n",
    "\n",
    "            if result is None:\n",
    "                query = sa.text(\"SELECT sys.fn_cdc_get_min_lsn(NULL)\")\n",
    "                return connection.execute(query).scalar()\n",
    "\n",
    "            return result\n",
    "\n",
    "    def lsn_to_hex_string(self, lsn_bytes):\n",
    "        \"\"\"Convert a binary LSN to a hex string format.\"\"\"\n",
    "        if lsn_bytes is None:\n",
    "            return LSN_DEFAULT\n",
    "\n",
    "        return f\"0x{lsn_bytes.hex().upper()}\"\n",
    "\n",
    "    def get_primary_key_columns(self, table_name: str) -> list[str]:\n",
    "        \"\"\"Get the primary key columns for a CDC-enabled table.\"\"\"\n",
    "        with self.get_connection() as connection:\n",
    "            instance = self.get_capture_instance_name(\"dbo\", table_name)\n",
    "\n",
    "            query = sa.text(\"\"\"\n",
    "            SELECT column_name FROM cdc.index_columns WHERE object_id = (\n",
    "                SELECT object_id FROM cdc.change_tables WHERE capture_instance = :capture_instance_name\n",
    "            )\n",
    "            \"\"\")\n",
    "\n",
    "            result = connection.execute(query, {\"capture_instance_name\": instance})\n",
    "            primary_key_columns = [row[0] for row in result]\n",
    "            return primary_key_columns\n",
    "\n",
    "    def get_merge_predicate(self, table_name: str) -> str:\n",
    "        \"\"\"Uses the primary key columns to construct a predicate for merging.\n",
    "        e.g., CustomerID and Email become: source.CustomerID = target.CustomerID AND source.Email = target.Email\n",
    "        \"\"\"\n",
    "        primary_key_columns = self.get_primary_key_columns(table_name)\n",
    "        if not primary_key_columns:\n",
    "            raise ValueError(f\"No primary key columns found for table {table_name}\")\n",
    "\n",
    "        # Construct the merge predicate\n",
    "        merge_predicate = \" AND \".join(\n",
    "            [f\"s.{col} = t.{col}\" for col in primary_key_columns]\n",
    "        )\n",
    "        return merge_predicate\n",
    "\n",
    "    def get_table_changes(\n",
    "            self, table_name, last_lsn=None, schema_name=\"dbo\", chunksize=10000\n",
    "        ) -> tuple[pl.DataFrame, str]:\n",
    "        \"\"\"Get changes from a CDC-enabled table since the last LSN.\n",
    "        Uses the native SQL Server CDC function fn_cdc_get_all_changes.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): The name of the table to query.\n",
    "            last_lsn (str, optional): The last processed LSN. If None, a full copy is performed.\n",
    "            schema_name (str, optional): The schema name of the table. Defaults to 'dbo'.\n",
    "            chunksize (int, optional): Number of rows to fetch per query. Defaults to 10000.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the DataFrame of changes and the current LSN.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.get_connection() as connection:\n",
    "                # Check if CDC is enabled for the database and table\n",
    "                if not self.is_cdc_enabled_for_database():\n",
    "                    raise ValueError(\n",
    "                        f\"CDC is not enabled for database {self.config.database.get_value()}\"\n",
    "                    )\n",
    "\n",
    "                if not self.is_cdc_enabled_for_table(schema_name, table_name):\n",
    "                    raise ValueError(\n",
    "                        f\"CDC not enabled for table {schema_name}.{table_name}\"\n",
    "                    )\n",
    "\n",
    "                # Get the capture instance name\n",
    "                capture_instance = self.get_capture_instance_name(\n",
    "                    schema_name, table_name\n",
    "                )\n",
    "                if not capture_instance:\n",
    "                    raise ValueError(\n",
    "                        f\"Could not find CDC capture instance for {schema_name}.{table_name}\"\n",
    "                    )\n",
    "\n",
    "                # Get current maximum LSN\n",
    "                current_lsn = self.get_current_lsn()\n",
    "                current_lsn_hex = self.lsn_to_hex_string(current_lsn)\n",
    "                \n",
    "                # If no last_lsn provided, we should first take a first copy of the table\n",
    "                if last_lsn is None or last_lsn == LSN_DEFAULT:\n",
    "                    raise ValueError(\n",
    "                        f\"Initial copy required for table {schema_name}.{table_name}\"\n",
    "                    )\n",
    "\n",
    "                # Convert LSN hex strings to binary\n",
    "                from_lsn_hex = last_lsn\n",
    "                to_lsn_hex = f\"0x{current_lsn.hex()}\"\n",
    "\n",
    "                # Use the native CDC function with parameterized query\n",
    "                # Process in chunks to avoid memory issues with large tables\n",
    "                query = sa.text(f\"\"\"\n",
    "                    DECLARE @from_lsn BINARY(10), @to_lsn BINARY(10)\n",
    "                    SET @from_lsn = CONVERT(BINARY(10), :from_lsn, 1)\n",
    "                    SET @to_lsn = CONVERT(BINARY(10), :to_lsn, 1)\n",
    "        \n",
    "                    SELECT * FROM cdc.fn_cdc_get_all_changes_{capture_instance}(\n",
    "                        @from_lsn, @to_lsn, 'all'\n",
    "                    )\n",
    "                \"\"\")\n",
    "\n",
    "                # Use chunksize to process large result sets in batches\n",
    "                changes_df = pl.read_database(\n",
    "                    query,\n",
    "                    connection,\n",
    "                    execute_options={\n",
    "                        \"parameters\": {\"from_lsn\": from_lsn_hex, \"to_lsn\": to_lsn_hex}\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # Convert binary LSN to hex string for storage\n",
    "                current_lsn_hex = self.lsn_to_hex_string(current_lsn)\n",
    "\n",
    "                return changes_df, current_lsn_hex\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Database error when getting CDC changes: {str(e)}\"\n",
    "            ) from e\n",
    "\n",
    "# Connect to SQL Server\n",
    "sql_resource = SQLResource(\n",
    "    SQL_HOST,\n",
    "    SQL_DATABASE,\n",
    "    SQL_USER,\n",
    "    SQL_PASS\n",
    ")\n",
    "\n",
    "last_lsn = \"0x0000004400000D280005\"\n",
    "\n",
    "changes = sql_resource.get_table_changes(\n",
    "    table_name=TABLE_NAME_REMOTE,\n",
    "    last_lsn=last_lsn, # todo: fetch this each time and save into metadata\n",
    "    schema_name=\"dbo\"\n",
    ")\n",
    "\n",
    "# Make them available as temporary view\n",
    "print(type(changes[0]))\n",
    "changes_df = spark.createDataFrame(changes[0].to_pandas())\n",
    "changes_df.createOrReplaceTempView(\"changes\")\n",
    "\n",
    "# print(changes)\n",
    "print(spark.sql(\"SELECT * FROM changes\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f0e2e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s.CustomerID = t.CustomerID\n"
     ]
    }
   ],
   "source": [
    "# Get the CDC Table Indices\n",
    "merge_predicate = sql_resource.get_merge_predicate(\n",
    "    table_name=TABLE_NAME_REMOTE\n",
    ")\n",
    "\n",
    "print(merge_predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a82fcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing merge operation on 'local.db.customers' with predicate 's.CustomerID = t.CustomerID'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the CDC changes into the iceberg table as merge\n",
    "# we work with the __$operation column to determine the type of operation, which can have values:\n",
    "# - Deleted (__$operation = 1),\n",
    "# - Inserted (__$operation = 2)\n",
    "# - Updated Before (__$operation = 3)\n",
    "# - Updated After (__$operation = 4)\n",
    "# https://iceberg.apache.org/docs/1.5.0/spark-writes/#merge-into\n",
    "print(f\"Performing merge operation on '{TABLE_NAME_LOCAL}' with predicate '{merge_predicate}'...\")\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {TABLE_NAME_LOCAL} AS t\n",
    "USING (SELECT * FROM changes) AS s\n",
    "ON {merge_predicate}\n",
    "WHEN MATCHED AND s.`__$operation` = 1 THEN DELETE\n",
    "WHEN MATCHED AND s.`__$operation` IN (2, 4) THEN UPDATE SET *\n",
    "-- Anything we can't match, we insert\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b79cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      13|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME_LOCAL}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "44951f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+--------------------+--------------------+\n",
      "|CustomerID|FirstName|LastName|               Email|         CreatedDate|         UpdatedDate|\n",
      "+----------+---------+--------+--------------------+--------------------+--------------------+\n",
      "|        29|  Michael|  Taylor|michael.taylor@ex...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|        30|    Sarah| Johnson|sarah.johnson@exa...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|        31|    David|  Miller|david.miller@exam...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|        32|     Emma|  Wilson|emma.wilson@examp...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|        33|   Thomas|Anderson|thomas.anderson@e...|2025-04-03 14:05:...|2025-04-03 14:05:...|\n",
      "|        28|    Brave|   Sunny|brave.sunny@examp...|2025-04-03 13:55:...|2025-04-03 13:55:...|\n",
      "|        23|  Michael|  Taylor|michael.taylor@ex...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|        24|    Sarah| Johnson|sarah.johnson@exa...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|        25|    David|  Miller|david.miller@exam...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|        26|     Emma|  Wilson|emma.wilson@examp...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|        27|   Thomas|Anderson|thomas.anderson@e...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|        24|    Sarah| Johnson|sarah.johnson@exa...|2025-04-03 13:36:...|2025-04-03 13:36:...|\n",
      "|        18|    Sarah| Johnson|sarah.johnson@exa...|2025-03-31 12:07:...|2025-03-31 12:07:...|\n",
      "+----------+---------+--------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {TABLE_NAME_LOCAL} ORDER BY UpdatedDate DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_lsn = sql_resource.get_current_lsn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fad0cdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC data cleared for Customers.\n"
     ]
    }
   ],
   "source": [
    "# Clean the CDC table\n",
    "# spark.sql(f\"TRUNCATE TABLE {TABLE_NAME_LOCAL}\")\n",
    "sql_resource.cdc_clear(TABLE_NAME_REMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ec300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
